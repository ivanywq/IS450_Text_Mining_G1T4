{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from joblib import dump\n",
    "import spacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rf/lkrb6ggn1vb7phs962gq11cm0000gn/T/ipykernel_49554/2510166184.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:279.)\n",
      "  tokens_tensor = torch.tensor([indexed_tokens])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9953900709219858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.990801393290076\n",
      "Recall: 0.9953900709219858\n",
      "F1 Score: 0.9930904315187539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.99667553 0.9964539  0.99667479 0.99667479 0.99667479]\n",
      "Average Score:  0.9966307634922357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bert_model.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_event_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    event_sentences = [sent.text.strip() for sent in doc.sents if any(ent.label_ in ['EVENT', 'DATE', 'TIME'] for ent in sent.ents)]\n",
    "    return ' '.join(event_sentences)\n",
    "\n",
    "# Function to calculate the embeddings\n",
    "def calculate_embeddings(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Define the maximum length of sequences\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    # Truncate and pad the input sequences so that they all have the same length\n",
    "    indexed_tokens = pad_sequences([indexed_tokens], maxlen=MAX_LEN, dtype=\"long\", \n",
    "                            value=0, truncating=\"post\", padding=\"post\")[0]\n",
    "\n",
    "    # Mark each of the tokens as belonging to sentence \"1\".\n",
    "    segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on how it's  configured in the `from_pretrained` call earlier. In this case, becase we set `output_hidden_states = True`, the third item will be the hidden states from all layers.\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        \n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    # Calculate the average embedding.\n",
    "    sentence_embedding = torch.mean(torch.stack(token_vecs_sum), dim=0)\n",
    "\n",
    "    return sentence_embedding.numpy()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Split the text by space\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove '##' and join subwords\n",
    "    clean_tokens = [token.replace('##', '') if token.startswith('##') else ' ' + token for token in tokens]\n",
    "\n",
    "    # Join tokens into a string to form the cleaned text\n",
    "    cleaned_text = ''.join(clean_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def tokens_to_sentence(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(indexed_tokens)\n",
    "\n",
    "    # Remove [CLS] and [SEP] tokens\n",
    "    tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "    # Join tokens into a string to form the sentence\n",
    "    sentence = ' '.join(tokens)\n",
    "\n",
    "    return sentence\n",
    "# Load your data\n",
    "df = pd.read_csv('news_cleaned_no_spaces.csv', encoding='latin1')\n",
    "df['news_text'] = df['news_text'].astype(str)\n",
    "\n",
    "# Extract event sentences\n",
    "df['output_text'] = df['news_text'].apply(extract_event_sentences)\n",
    "\n",
    "# Calculate the embeddings for each sentence\n",
    "df['output'] = df['output_text'].apply(calculate_embeddings)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(df['output'].tolist(), df['gold_truth'].tolist(), test_size=0.2, random_state=30)\n",
    "\n",
    "# ------- TRAIN CLASSIFIER ------------\n",
    "# Convert list of arrays into a 2D array\n",
    "train_sentences_array = np.vstack(train_sentences)\n",
    "test_sentences_array = np.vstack(test_sentences)\n",
    "\n",
    "# Train a Support Vector Machine model\n",
    "classifier = SVC(random_state=42)\n",
    "classifier.fit(train_sentences_array, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = classifier.predict(test_sentences_array)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Calculate the precision\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "# Calculate the recall\n",
    "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "scores = cross_val_score(classifier, train_sentences_array, train_labels, cv=5)\n",
    "print(\"Cross-Validation Scores: \", scores)\n",
    "print(\"Average Score: \", scores.mean())\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Convert token IDs back to sentences and store in 'output_sentence' column\n",
    "df['output_sentence'] = df['output_text'].apply(tokens_to_sentence)\n",
    "\n",
    "# Clean text\n",
    "df['cleaned_text'] = df['output_sentence'].apply(clean_text)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f'predicted_sentences_bert{timestamp}.csv', index=False)\n",
    "\n",
    "# Save the model\n",
    "dump(classifier, 'bert_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('bert_model.pkl', 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ivan/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/rf/lkrb6ggn1vb7phs962gq11cm0000gn/T/ipykernel_76635/2458791411.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:279.)\n",
      "  tokens_tensor = torch.tensor([indexed_tokens])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9953900709219858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.990801393290076\n",
      "Recall: 0.9953900709219858\n",
      "F1 Score: 0.9930904315187539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.99667553 0.9964539  0.99667479 0.99667479 0.99667479]\n",
      "Average Score:  0.9966307634922357\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from joblib import dump\n",
    "import spacy\n",
    "import pickle\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_event_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    event_sentences = [sent.text.strip() for sent in doc.sents if any(ent.label_ in ['EVENT', 'DATE', 'TIME'] for ent in sent.ents)]\n",
    "    return ' '.join(event_sentences)\n",
    "\n",
    "# Function to calculate the embeddings\n",
    "def calculate_embeddings(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Define the maximum length of sequences\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    # Truncate and pad the input sequences so that they all have the same length\n",
    "    indexed_tokens = pad_sequences([indexed_tokens], maxlen=MAX_LEN, dtype=\"long\", \n",
    "                            value=0, truncating=\"post\", padding=\"post\")[0]\n",
    "\n",
    "    # Mark each of the tokens as belonging to sentence \"1\".\n",
    "    segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on how it's  configured in the `from_pretrained` call earlier. In this case, becase we set `output_hidden_states = True`, the third item will be the hidden states from all layers.\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        \n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    # Calculate the average embedding.\n",
    "    sentence_embedding = torch.mean(torch.stack(token_vecs_sum), dim=0)\n",
    "\n",
    "    return sentence_embedding.numpy()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Split the text by space\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove '##' and join subwords\n",
    "    clean_tokens = [token.replace('##', '') if token.startswith('##') else ' ' + token for token in tokens]\n",
    "\n",
    "    # Join tokens into a string to form the cleaned text\n",
    "    cleaned_text = ''.join(clean_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def tokens_to_sentence(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(indexed_tokens)\n",
    "\n",
    "    # Remove [CLS] and [SEP] tokens\n",
    "    tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "    # Join tokens into a string to form the sentence\n",
    "    sentence = ' '.join(tokens)\n",
    "\n",
    "    return sentence\n",
    "# Load your data\n",
    "df = pd.read_csv('news_cleaned_no_spaces.csv', encoding='latin1')\n",
    "df['news_text'] = df['news_text'].astype(str)\n",
    "\n",
    "# Extract event sentences\n",
    "df['output_text'] = df['news_text'].apply(extract_event_sentences)\n",
    "\n",
    "# Calculate the embeddings for each sentence\n",
    "df['output'] = df['output_text'].apply(calculate_embeddings)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(df['output'].tolist(), df['gold_truth'].tolist(), test_size=0.2, random_state=30)\n",
    "\n",
    "# ------- TRAIN CLASSIFIER ------------\n",
    "# Convert list of arrays into a 2D array\n",
    "train_sentences_array = np.vstack(train_sentences)\n",
    "test_sentences_array = np.vstack(test_sentences)\n",
    "\n",
    "# Train a Support Vector Machine model\n",
    "classifier = SVC(random_state=42)\n",
    "classifier.fit(train_sentences_array, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = classifier.predict(test_sentences_array)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Calculate the precision\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "# Calculate the recall\n",
    "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "scores = cross_val_score(classifier, train_sentences_array, train_labels, cv=5)\n",
    "print(\"Cross-Validation Scores: \", scores)\n",
    "print(\"Average Score: \", scores.mean())\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Convert token IDs back to sentences and store in 'output_sentence' column\n",
    "df['output_sentence'] = df['output_text'].apply(tokens_to_sentence)\n",
    "\n",
    "# Clean text\n",
    "df['cleaned_text'] = df['output_sentence'].apply(clean_text)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f'predicted_sentences_bert{timestamp}.csv', index=False)\n",
    "\n",
    "# Save the model\n",
    "with open('bert_model.pkl', 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open('bert_model.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have a new input text\n",
    "input_text = input(\"Enter your text here:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5dca89291d479d9ab93025360a5c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Input:', placeholder='Enter your text here:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "# Create text box for input\n",
    "input_text = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter your text here:',\n",
    "    description='Input:',\n",
    "    disable=False\n",
    ")\n",
    "\n",
    "# Display the text box\n",
    "display(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology companies are known for strong revenue growth fueled by their innovations, but that doesn't always translate to the bottom line. A number of tech companies are not profitable, but profits are a must in order to pay dividends. Otherwise, it should raise questions over the affordability of that dividend. For example, data storage provider Seagate Technology (STX -1.19%) delivered an attractive yield of 3.7% at the time of this writing. But look past that juicy yield at the company's financials, and it's not a pretty picture. In its fiscal first quarter, ended Sept. 29, Seagate paid out dividends of $145 million but suffered a net loss of $184 million. The company also generated free cash flow (FCF) of $57 million. FCF provides insight into the cash available for a company to invest in its business, pay debt obligations, repurchase shares, and hand out dividends. With no profit and a dividend payout more than double its FCF, Seagate can't sustain a payout if its financials don't improve. Fortunately, plenty of tech companies with excellent financial health pay dividends. Here's a look at three that can deliver dependable passive income: Cisco Systems (CSCO 1.23%), IBM (IBM -0.30%), and Verizon Communications (VZ -0.05%). Cisco: 3.3% dividend yield Tech veteran Cisco is a compelling dividend stock for several reasons. It began as a computer networking company, then expanded into a variety of software businesses, such as cybersecurity. The expansion from networking hardware into software enabled Cisco to implement a software-as-a-service (SaaS) subscription model. This gives it recurring revenue, adding to its ability to pay its dividend, and helping grow its business. Its fiscal first quarter, ended Oct. 28, was the strongest first quarter in company history. Revenue increased 8% year over year to $14.7 billion, and net income rose 36% year over year to $3.6 billion. That performance was an extension of Cisco's strong fiscal 2023, ended July 29, in which year-over-year revenue jumped 11% to $57 billion, and net income increased 7% to $12.6 billion. With its strong financials, the company can deliver a solid dividend yield of 3.3%. Cisco raised its dividend in 2023 for the 13th consecutive year, a good track record of increases. The dividend payout ratio, which tells you the percentage of a company's earnings used for its payout, was 47%, leaving plenty of cash to reinvest in its business. For instance, Cisco recently announced it was buying Splunk, a cybersecurity analytics firm. This strengthens the company's cybersecurity offerings while adding Splunk's growing revenue to its financials, further improving Cisco's ability to pay its dividend. IBM: 4.3% dividend yield Venerable IBM delivers a dependable dividend. Big Blue provided a payout since 1916 with an impressive 28-year streak of consecutive annual increases. On top of that, the company offers a robust dividend yield of 4.3%. It's well positioned to fund its dividend. In 2020, IBM shifted its focus to the red-hot industries of cloud computing and artificial intelligence, enabling it to steadily increase revenue since that time. IBM is now one of the top 10 cloud computing companies in the world, and its success shows in its results. In the third quarter, it generated sales of $14.8 billion, a year-over-year increase of 5%. Net income and FCF both hit $1.7 billion. Its dividend payout ratio is on the high side at 87%, but its $10.3 billion in FCF over the trailing 12 months comfortably covered its dividend payments of $6 billion over that period. IBM expects to end 2023 with $10.5 billion in FCF, illustrating its consistency in generating free cash flow. Verizon: 7.3% dividend yield This telecom is tops among the companies on this list when it comes to dividend yield, currently over 7%. Verizon raised its payout for the 17th consecutive year in September, giving it a solid track record of increases. The dependable dividend is thanks to the bulk of its revenue coming from wireless subscribers, a reliable source given our dependency on mobile phones today. Third-quarter wireless revenue grew 3% year over year, accounting for $19.3 billion of the $33.3 billion in total sales. Verizon also consistently generates strong free cash flow, which reached $6.7 billion in the third quarter. Year to date, it stands at $14.6 billion, $2.2 billion higher than 2022, and it already surpasses 2022's full-year FCF of $14.1 billion. As a result, Verizon raised its 2023 FCF forecast by $1 billion to $18 billion. The dividend payout ratio of 53% provides room to pay down debt and invest in its new, more powerful 5G network, which should be a source of revenue and free cash flow growth for years. Thanks to solid financials, Verizon, Cisco, and IBM are three great income stocks to add to your portfolio.\n",
      "Event Sentences: \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology companies are known for strong revenue growth fueled by their innovations, but that doesn't always translate to the bottom line. A number of tech companies are not profitable, but profits are a must in order to pay dividends. Otherwise, it should raise questions over the affordability of that dividend. For example, data storage provider Seagate Technology (STX -1.19%) delivered an attractive yield of 3.7% at the time of this writing. But look past that juicy yield at the company's financials, and it's not a pretty picture. In its fiscal first quarter, ended Sept. 29, Seagate paid out dividends of $145 million but suffered a net loss of $184 million. The company also generated free cash flow (FCF) of $57 million. FCF provides insight into the cash available for a company to invest in its business, pay debt obligations, repurchase shares, and hand out dividends. With no profit and a dividend payout more than double its FCF, Seagate can't sustain a payout if its financials don't improve. Fortunately, plenty of tech companies with excellent financial health pay dividends. Here's a look at three that can deliver dependable passive income: Cisco Systems (CSCO 1.23%), IBM (IBM -0.30%), and Verizon Communications (VZ -0.05%). Cisco: 3.3% dividend yield Tech veteran Cisco is a compelling dividend stock for several reasons. It began as a computer networking company, then expanded into a variety of software businesses, such as cybersecurity. The expansion from networking hardware into software enabled Cisco to implement a software-as-a-service (SaaS) subscription model. This gives it recurring revenue, adding to its ability to pay its dividend, and helping grow its business. Its fiscal first quarter, ended Oct. 28, was the strongest first quarter in company history. Revenue increased 8% year over year to $14.7 billion, and net income rose 36% year over year to $3.6 billion. That performance was an extension of Cisco's strong fiscal 2023, ended July 29, in which year-over-year revenue jumped 11% to $57 billion, and net income increased 7% to $12.6 billion. With its strong financials, the company can deliver a solid dividend yield of 3.3%. Cisco raised its dividend in 2023 for the 13th consecutive year, a good track record of increases. The dividend payout ratio, which tells you the percentage of a company's earnings used for its payout, was 47%, leaving plenty of cash to reinvest in its business. For instance, Cisco recently announced it was buying Splunk, a cybersecurity analytics firm. This strengthens the company's cybersecurity offerings while adding Splunk's growing revenue to its financials, further improving Cisco's ability to pay its dividend. IBM: 4.3% dividend yield Venerable IBM delivers a dependable dividend. Big Blue provided a payout since 1916 with an impressive 28-year streak of consecutive annual increases. On top of that, the company offers a robust dividend yield of 4.3%. It's well positioned to fund its dividend. In 2020, IBM shifted its focus to the red-hot industries of cloud computing and artificial intelligence, enabling it to steadily increase revenue since that time. IBM is now one of the top 10 cloud computing companies in the world, and its success shows in its results. In the third quarter, it generated sales of $14.8 billion, a year-over-year increase of 5%. Net income and FCF both hit $1.7 billion. Its dividend payout ratio is on the high side at 87%, but its $10.3 billion in FCF over the trailing 12 months comfortably covered its dividend payments of $6 billion over that period. IBM expects to end 2023 with $10.5 billion in FCF, illustrating its consistency in generating free cash flow. Verizon: 7.3% dividend yield This telecom is tops among the companies on this list when it comes to dividend yield, currently over 7%. Verizon raised its payout for the 17th consecutive year in September, giving it a solid track record of increases. The dependable dividend is thanks to the bulk of its revenue coming from wireless subscribers, a reliable source given our dependency on mobile phones today. Third-quarter wireless revenue grew 3% year over year, accounting for $19.3 billion of the $33.3 billion in total sales. Verizon also consistently generates strong free cash flow, which reached $6.7 billion in the third quarter. Year to date, it stands at $14.6 billion, $2.2 billion higher than 2022, and it already surpasses 2022's full-year FCF of $14.1 billion. As a result, Verizon raised its 2023 FCF forecast by $1 billion to $18 billion. The dividend payout ratio of 53% provides room to pay down debt and invest in its new, more powerful 5G network, which should be a source of revenue and free cash flow growth for years. Thanks to solid financials, Verizon, Cisco, and IBM are three great income stocks to add to your portfolio.\n",
      "Event sentences:   in its fiscal first quarter , ended sept . 29 , seagate paid out dividends of $ 145 million but suffered a net loss of $ 184 million . its fiscal first quarter , ended oct . 28 , was the strongest first quarter in company history . revenue increased 8 % year over year to $ 14 . 7 billion , and net income rose 36 % year over year to $ 3 . 6 billion . that performance was an extension of cisco ' s strong fiscal 2023 , ended july 29 , in which year - over - year revenue jumped 11 % to $ 57 billion , and net income increased 7 % to $ 12 . 6 billion . cisco raised its dividend in 2023 for the 13th consecutive year , a good track record of increases . big blue provided a payout since 1916 with an impressive 28 - year streak of consecutive annual increases . in 2020 , ibm shifted its focus to the red - hot industries of cloud computing and artificial intelligence , enabling it to steadily increase revenue since that time . in the third quarter , it generated sales of $ 14 . 8 billion , a year - over - year increase of 5 % . its dividend payout ratio is on the high side at 87 % , but its $ 10 . 3 billion in fcf over the trailing 12 months comfortably covered its dividend payments of $ 6 billion over that period . ibm expects to end 2023 with $ 10 . 5 billion in fcf , illustrating its consistency in generating free cash flow . verizon raised its payout for the 17th consecutive year in september , giving it a solid track record of increases . the dependable dividend is thanks to the bulk of its revenue coming from wireless subscribers , a reliable source given our dependency on mobile phones today . third - quarter wireless revenue grew 3 % year over year , accounting for $ 19 . 3 billion of the $ 33 . 3 billion in total sales . verizon also consistently generates strong free cash flow , which reached $ 6 . 7 billion in the third quarter . year to date , it stands at $ 14 . 6 billion , $ 2 . 2 billion higher than 2022 , and it already surpasses 2022 ' s full - year fcf of $ 14 . 1 billion . the dividend payout ratio of 53 % provides room to pay down debt and invest in its new , more powerful 5g network , which should be a source of revenue and free cash flow growth for years .\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load the trained model\n",
    "with open('bert_model.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "# Define the functions: extract_event_sentences, calculate_embeddings, clean_text, tokens_to_sentence\n",
    "def extract_event_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    event_sentences = [sent.text.strip() for sent in doc.sents if any(ent.label_ in ['EVENT', 'DATE', 'TIME'] for ent in sent.ents)]\n",
    "    return ' '.join(event_sentences)\n",
    "\n",
    "# Function to calculate the embeddings\n",
    "def calculate_embeddings(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Define the maximum length of sequences\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    # Truncate and pad the input sequences so that they all have the same length\n",
    "    indexed_tokens = pad_sequences([indexed_tokens], maxlen=MAX_LEN, dtype=\"long\", \n",
    "                            value=0, truncating=\"post\", padding=\"post\")[0]\n",
    "\n",
    "    # Mark each of the tokens as belonging to sentence \"1\".\n",
    "    segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on how it's  configured in the `from_pretrained` call earlier. In this case, becase we set `output_hidden_states = True`, the third item will be the hidden states from all layers.\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        \n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    # Calculate the average embedding.\n",
    "    sentence_embedding = torch.mean(torch.stack(token_vecs_sum), dim=0)\n",
    "\n",
    "    return sentence_embedding.numpy()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Split the text by space\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove '##' and join subwords\n",
    "    clean_tokens = [token.replace('##', '') if token.startswith('##') else ' ' + token for token in tokens]\n",
    "\n",
    "    # Join tokens into a string to form the cleaned text\n",
    "    cleaned_text = ''.join(clean_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def tokens_to_sentence(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(indexed_tokens)\n",
    "\n",
    "    # Remove [CLS] and [SEP] tokens\n",
    "    tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "    # Join tokens into a string to form the sentence\n",
    "    sentence = ' '.join(tokens)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# Now, you can use the model to predict the events from a new input text\n",
    "def predict_events(input_text):\n",
    "    # Extract event sentences\n",
    "    event_sentences = extract_event_sentences(input_text)\n",
    "\n",
    "    # Calculate the embeddings for the event sentences\n",
    "    embeddings = calculate_embeddings(event_sentences)\n",
    "\n",
    "    # Convert token IDs back to sentences\n",
    "    output_sentence = tokens_to_sentence(event_sentences)\n",
    "\n",
    "    # Use the trained model to predict the events\n",
    "    prediction = classifier.predict([embeddings])\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(output_sentence)\n",
    "    return cleaned_text, prediction\n",
    "\n",
    "# Test the function with a new input text\n",
    "print(input_text.value)\n",
    "cleaned_text, prediction = predict_events(input_text.value)\n",
    "print(\"Event sentences: \",cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
