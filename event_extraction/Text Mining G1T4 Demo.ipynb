{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Extraction - Solution 3: BERT, spaCy, SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rf/lkrb6ggn1vb7phs962gq11cm0000gn/T/ipykernel_49554/2510166184.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:279.)\n",
      "  tokens_tensor = torch.tensor([indexed_tokens])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9953900709219858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.990801393290076\n",
      "Recall: 0.9953900709219858\n",
      "F1 Score: 0.9930904315187539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.99667553 0.9964539  0.99667479 0.99667479 0.99667479]\n",
      "Average Score:  0.9966307634922357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bert_model.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from joblib import dump\n",
    "import spacy\n",
    "import pickle\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_event_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    event_sentences = [sent.text.strip() for sent in doc.sents if any(ent.label_ in ['EVENT', 'DATE', 'TIME'] for ent in sent.ents)]\n",
    "    return ' '.join(event_sentences)\n",
    "\n",
    "# Function to calculate the embeddings\n",
    "def calculate_embeddings(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Define the maximum length of sequences\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    # Truncate and pad the input sequences so that they all have the same length\n",
    "    indexed_tokens = pad_sequences([indexed_tokens], maxlen=MAX_LEN, dtype=\"long\", \n",
    "                            value=0, truncating=\"post\", padding=\"post\")[0]\n",
    "\n",
    "    # Mark each of the tokens as belonging to sentence \"1\".\n",
    "    segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on how it's  configured in the `from_pretrained` call earlier. In this case, becase we set `output_hidden_states = True`, the third item will be the hidden states from all layers.\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        \n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    # Calculate the average embedding.\n",
    "    sentence_embedding = torch.mean(torch.stack(token_vecs_sum), dim=0)\n",
    "\n",
    "    return sentence_embedding.numpy()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Split the text by space\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove '##' and join subwords\n",
    "    clean_tokens = [token.replace('##', '') if token.startswith('##') else ' ' + token for token in tokens]\n",
    "\n",
    "    # Join tokens into a string to form the cleaned text\n",
    "    cleaned_text = ''.join(clean_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def tokens_to_sentence(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(indexed_tokens)\n",
    "\n",
    "    # Remove [CLS] and [SEP] tokens\n",
    "    tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "    # Join tokens into a string to form the sentence\n",
    "    sentence = ' '.join(tokens)\n",
    "\n",
    "    return sentence\n",
    "# Load your data\n",
    "df = pd.read_csv('news_cleaned_no_spaces.csv', encoding='latin1')\n",
    "df['news_text'] = df['news_text'].astype(str)\n",
    "\n",
    "# Extract event sentences\n",
    "df['output_text'] = df['news_text'].apply(extract_event_sentences)\n",
    "\n",
    "# Calculate the embeddings for each sentence\n",
    "df['output'] = df['output_text'].apply(calculate_embeddings)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(df['output'].tolist(), df['gold_truth'].tolist(), test_size=0.2, random_state=30)\n",
    "\n",
    "# ------- TRAIN CLASSIFIER ------------\n",
    "# Convert list of arrays into a 2D array\n",
    "train_sentences_array = np.vstack(train_sentences)\n",
    "test_sentences_array = np.vstack(test_sentences)\n",
    "\n",
    "# Train a Support Vector Machine model\n",
    "classifier = SVC(random_state=42)\n",
    "classifier.fit(train_sentences_array, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = classifier.predict(test_sentences_array)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Calculate the precision\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "# Calculate the recall\n",
    "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "scores = cross_val_score(classifier, train_sentences_array, train_labels, cv=5)\n",
    "print(\"Cross-Validation Scores: \", scores)\n",
    "print(\"Average Score: \", scores.mean())\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Convert token IDs back to sentences and store in 'output_sentence' column\n",
    "df['output_sentence'] = df['output_text'].apply(tokens_to_sentence)\n",
    "\n",
    "# Clean text\n",
    "df['cleaned_text'] = df['output_sentence'].apply(clean_text)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f'predicted_sentences_bert{timestamp}.csv', index=False)\n",
    "\n",
    "# Save the model\n",
    "dump(classifier, 'bert_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ivan/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from joblib import dump\n",
    "import spacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan/anaconda3/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.4.1.post1 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "with open('bert_model.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Technology companies are known for strong revenue growth fueled by their innovations, but that doesn't always translate to the bottom line. A number of tech companies are not profitable, but profits are a must in order to pay dividends. Otherwise, it should raise questions over the affordability of that dividend. For example, data storage provider Seagate Technology (STX -1.19%) delivered an attractive yield of 3.7% at the time of this writing. But look past that juicy yield at the company's financials, and it's not a pretty picture. In its fiscal first quarter, ended Sept. 29, Seagate paid out dividends of $145 million but suffered a net loss of $184 million. The company also generated free cash flow (FCF) of $57 million. FCF provides insight into the cash available for a company to invest in its business, pay debt obligations, repurchase shares, and hand out dividends. With no profit and a dividend payout more than double its FCF, Seagate can't sustain a payout if its financials don't improve. Fortunately, plenty of tech companies with excellent financial health pay dividends. Here's a look at three that can deliver dependable passive income: Cisco Systems (CSCO 1.23%), IBM (IBM -0.30%), and Verizon Communications (VZ -0.05%). Cisco: 3.3% dividend yield Tech veteran Cisco is a compelling dividend stock for several reasons. It began as a computer networking company, then expanded into a variety of software businesses, such as cybersecurity. The expansion from networking hardware into software enabled Cisco to implement a software-as-a-service (SaaS) subscription model. This gives it recurring revenue, adding to its ability to pay its dividend, and helping grow its business. Its fiscal first quarter, ended Oct. 28, was the strongest first quarter in company history. Revenue increased 8% year over year to $14.7 billion, and net income rose 36% year over year to $3.6 billion. That performance was an extension of Cisco's strong fiscal 2023, ended July 29, in which year-over-year revenue jumped 11% to $57 billion, and net income increased 7% to $12.6 billion. With its strong financials, the company can deliver a solid dividend yield of 3.3%. Cisco raised its dividend in 2023 for the 13th consecutive year, a good track record of increases. The dividend payout ratio, which tells you the percentage of a company's earnings used for its payout, was 47%, leaving plenty of cash to reinvest in its business. For instance, Cisco recently announced it was buying Splunk, a cybersecurity analytics firm. This strengthens the company's cybersecurity offerings while adding Splunk's growing revenue to its financials, further improving Cisco's ability to pay its dividend. IBM: 4.3% dividend yield Venerable IBM delivers a dependable dividend. Big Blue provided a payout since 1916 with an impressive 28-year streak of consecutive annual increases. On top of that, the company offers a robust dividend yield of 4.3%. It's well positioned to fund its dividend. In 2020, IBM shifted its focus to the red-hot industries of cloud computing and artificial intelligence, enabling it to steadily increase revenue since that time. IBM is now one of the top 10 cloud computing companies in the world, and its success shows in its results. In the third quarter, it generated sales of $14.8 billion, a year-over-year increase of 5%. Net income and FCF both hit $1.7 billion. Its dividend payout ratio is on the high side at 87%, but its $10.3 billion in FCF over the trailing 12 months comfortably covered its dividend payments of $6 billion over that period. IBM expects to end 2023 with $10.5 billion in FCF, illustrating its consistency in generating free cash flow. Verizon: 7.3% dividend yield This telecom is tops among the companies on this list when it comes to dividend yield, currently over 7%. Verizon raised its payout for the 17th consecutive year in September, giving it a solid track record of increases. The dependable dividend is thanks to the bulk of its revenue coming from wireless subscribers, a reliable source given our dependency on mobile phones today. Third-quarter wireless revenue grew 3% year over year, accounting for $19.3 billion of the $33.3 billion in total sales. Verizon also consistently generates strong free cash flow, which reached $6.7 billion in the third quarter. Year to date, it stands at $14.6 billion, $2.2 billion higher than 2022, and it already surpasses 2022's full-year FCF of $14.1 billion. As a result, Verizon raised its 2023 FCF forecast by $1 billion to $18 billion. The dividend payout ratio of 53% provides room to pay down debt and invest in its new, more powerful 5G network, which should be a source of revenue and free cash flow growth for years. Thanks to solid financials, Verizon, Cisco, and IBM are three great income stocks to add to your portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25aed72d6c1341de869ccddef6e5994c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Input:', placeholder='Enter your text here:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create text box for input\n",
    "import ipywidgets as widgets\n",
    "\n",
    "input_text = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter your text here:',\n",
    "    description='Input:',\n",
    "    disable=False\n",
    ")\n",
    "\n",
    "# Display the text box\n",
    "display(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load the trained model\n",
    "with open('bert_model.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "# Define the functions: extract_event_sentences, calculate_embeddings, clean_text, tokens_to_sentence\n",
    "def extract_event_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    event_sentences = [sent.text.strip() for sent in doc.sents if any(ent.label_ in ['EVENT', 'DATE', 'TIME'] for ent in sent.ents)]\n",
    "    return ' '.join(event_sentences)\n",
    "\n",
    "# Function to calculate the embeddings\n",
    "def calculate_embeddings(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Define the maximum length of sequences\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    # Truncate and pad the input sequences so that they all have the same length\n",
    "    indexed_tokens = pad_sequences([indexed_tokens], maxlen=MAX_LEN, dtype=\"long\", \n",
    "                            value=0, truncating=\"post\", padding=\"post\")[0]\n",
    "\n",
    "    # Mark each of the tokens as belonging to sentence \"1\".\n",
    "    segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on how it's  configured in the `from_pretrained` call earlier. In this case, becase we set `output_hidden_states = True`, the third item will be the hidden states from all layers.\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        \n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    # Calculate the average embedding.\n",
    "    sentence_embedding = torch.mean(torch.stack(token_vecs_sum), dim=0)\n",
    "\n",
    "    return sentence_embedding.numpy()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Split the text by space\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove '##' and join subwords\n",
    "    clean_tokens = [token.replace('##', '') if token.startswith('##') else ' ' + token for token in tokens]\n",
    "\n",
    "    # Join tokens into a string to form the cleaned text\n",
    "    cleaned_text = ''.join(clean_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def tokens_to_sentence(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(indexed_tokens)\n",
    "\n",
    "    # Remove [CLS] and [SEP] tokens\n",
    "    tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "    # Join tokens into a string to form the sentence\n",
    "    sentence = ' '.join(tokens)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# Now, you can use the model to predict the events from a new input text\n",
    "def predict_events(input_text):\n",
    "    # Extract event sentences\n",
    "    event_sentences = extract_event_sentences(input_text)\n",
    "\n",
    "    # Calculate the embeddings for the event sentences\n",
    "    embeddings = calculate_embeddings(event_sentences)\n",
    "\n",
    "    # Convert token IDs back to sentences\n",
    "    output_sentence = tokens_to_sentence(event_sentences)\n",
    "\n",
    "    # Use the trained model to predict the events\n",
    "    prediction = classifier.predict([embeddings])\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(output_sentence)\n",
    "    return cleaned_text, prediction\n",
    "\n",
    "# Test the function with a new input text\n",
    "cleaned_text, prediction = predict_events(input_text.value)\n",
    "print(\"News Article:\\n\",input_text.value)\n",
    "print(\"\\n-------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Event sentences:\\n\",cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import yfinance\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Feature extraction functions----------------------------------\n",
    "\n",
    "def get_company_name(ticker):\n",
    "    \"\"\"\n",
    "    Get the full company name for a given stock ticker.\n",
    "    \n",
    "    Args:\n",
    "    - ticker (str): The stock ticker to look up.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The full company name for the stock ticker.\n",
    "    \"\"\"\n",
    "    company = ''\n",
    "    try:\n",
    "        company = yfinance.Ticker(ticker).info['longName']\n",
    "    except:\n",
    "        company = ''\n",
    "    return company\n",
    "\n",
    "\n",
    "def compiled_list_of_keywords(ticker, company_name):\n",
    "    \"\"\"\n",
    "    Generate a list of keywords to search for in news articles based on the stock ticker and company name.\n",
    "    \n",
    "    Args:\n",
    "    - ticker (str): The stock ticker to generate keywords for.\n",
    "    - company_name (str): The full company name to generate keywords for.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of keywords to search for in news articles.\n",
    "    \"\"\"\n",
    "    keywords = [ticker, company_name]\n",
    "    # Split the company name into individual words and add to the list of keywords\n",
    "    company_name = re.sub(r'[^a-zA-Z\\s]', '', company_name)  # Remove non-letter characters\n",
    "    keywords.extend(company_name.split())\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def find_relevant_sentences(text, keywords):\n",
    "    \"\"\"\n",
    "    Extract sentences from the provided text that contain any of the specified keywords.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The text to search within.\n",
    "    - keywords (list): A list of keywords to search for.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of sentences from the text that contain any of the keywords.\n",
    "    \"\"\"\n",
    "    relevant_sentences = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Convert keywords to lowercase for case-insensitive matching\n",
    "    keywords_lower = [keyword.lower() for keyword in keywords]\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        # Check if any keyword is in the sentence\n",
    "        if any(keyword in sent.text.lower() for keyword in keywords_lower):\n",
    "            relevant_sentences.append(sent.text)\n",
    "            \n",
    "    return relevant_sentences\n",
    "\n",
    "def process_keywords(keywords):\n",
    "    \"\"\"\n",
    "    Process the list of keywords to remove any duplicates and convert to lowercase.\n",
    "    \n",
    "    Args:\n",
    "    - keywords (list): A list of keywords to process.\n",
    "    \n",
    "    Returns:\n",
    "    - list: The processed list of keywords.\n",
    "    \"\"\"\n",
    "    keywords = [keyword.lower() for keyword in keywords]  # Convert to lowercase\n",
    "    for word in keywords:\n",
    "    # remove the word from keywords if it contains a non-alphabet character\n",
    "        if not word.isalpha():\n",
    "            keywords.remove(word)\n",
    "    \n",
    "    for word in keywords:\n",
    "        if \"inc\" in word or \"corporation\" in word:\n",
    "            keywords.remove(word)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "\n",
    "## Model Processing functions----------------------------------\n",
    "\n",
    "def aggregate_article_sentiment(sentiments, index):\n",
    "    \"\"\"\n",
    "    Aggregates sentiment scores from individual sentences to determine the overall article sentiment.\n",
    "\n",
    "    Parameters:\n",
    "    - sentiments (list of dicts): Each dict contains 'label' and 'score' for a sentence.\n",
    "\n",
    "    Returns:\n",
    "    - str: The overall sentiment of the article ('positive', 'neutral', 'negative').\n",
    "    \"\"\"\n",
    "    # Initialize counters for each sentiment\n",
    "    total_scores = {'positive': 0, 'neutral': 0, 'negative': 0}\n",
    "    \n",
    "    # Sum up the scores for each sentiment\n",
    "    for sentiment in sentiments:\n",
    "        label = sentiment['label']\n",
    "        score = sentiment['score']\n",
    "        if label in total_scores:\n",
    "            total_scores[label] += score\n",
    "    \n",
    "    # Normalize the scores to sum up to 1\n",
    "    total_score = sum(total_scores.values())\n",
    "    if total_score > 0:  # Avoid division by zero\n",
    "        for key in total_scores:\n",
    "            total_scores[key] /= total_score\n",
    "    \n",
    "    # Determine the overall sentiment by finding the max score\n",
    "    overall_sentiment = max(total_scores, key=total_scores.get)\n",
    "    print(f\"Overall Sentiment for row {index}: {overall_sentiment}\")\n",
    "    \n",
    "    return overall_sentiment\n",
    "\n",
    "\n",
    "def chunk_sentence(sentences, chunk_size):\n",
    "    \"\"\"\n",
    "    Chunk the list of sentences into smaller groups of a specified size.\n",
    "\n",
    "    Parameters:\n",
    "    - sentences (list): The list of sentences to chunk.\n",
    "    - chunk_size (int): The maximum number of sentences in each chunk.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of chunks, where each chunk is a list of sentences.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        chunks.append(sentences[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def processed_relevant_sentences(relevant_sentences):\n",
    "    \"\"\"\n",
    "    Process the list of relevant sentences by appending the chunks of sentences to the list\n",
    "\n",
    "    Parameters:\n",
    "    - relevant_sentences (list): A list of relevant sentences to process.\n",
    "\n",
    "    Returns:\n",
    "    - list: The processed list of relevant sentences.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for sentence in relevant_sentences:\n",
    "        chunks = chunk_sentence(sentence, 512)\n",
    "        for chunk in chunks:\n",
    "            result.append(chunk)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate FinBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142dcb7137e04596843cb3c1fbead31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30051fa5935b4746b27b855992a8ea98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3331f0c207f4864b3c3eb4caee58c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20221c55986641da80966b3d911e5444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82adbee48e9446ffafe8632b629c5423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/369 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Model Processing - GETTING SENTIMENT LABELS\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load spaCy model for sentence tokenization\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a news article, containing long texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to find the sentiment of this article about Cisco Systems, Inc. (CSCO)\n",
    "# This is an bullish article;\n",
    "news_article = '''\n",
    "Kyndryl Holdings Inc KD expanded its technology partnership with Cisco Systems Inc CSCO to offer cyber resilience services. \n",
    "Per this deal, KD will combine its cyber resilience framework with Cisco's extensive network software portfolio, hardware, and equipment to aid customers in efficiently addressing cyber incidents. \n",
    "Customers can maximize the return on their security investment through the adoption of more efficient and integrated security solutions. \n",
    "\"As customers consume more cloud-based applications, it's more important than ever to have the right tools to help them integrate a cyber resilient framework into their IT strategy and business operations. \n",
    "Our collaboration with Cisco will enable Kyndryl to support our customers' zero trust journeys with dynamic and tailored solutions while integrating existing security controls,\" said Michelle Weston, Vice President of Global Offerings for Security and Resiliency. \n",
    "Earlier this month, KD reported Q1 FY24 revenues of $4.2 billion and a quarterly net loss of $(141) million. The company also raised its fiscal 2024 adjusted EBITDA margin outlook to approximately 14%, up from its prior projection of 12% - 13%. \n",
    "Price Action: KD shares are trading flat at $15.99 on the last check Thursday.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CSCO', 'Cisco Systems', 'Cisco', 'Systems']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of keywords containing the stock ticker and company name\n",
    "# Ticker is provided by the dataset\n",
    "# Company name is obtained from Yahoo Finance\n",
    "ticker_companyName = compiled_list_of_keywords(\"CSCO\", \"Cisco Systems\")\n",
    "print(ticker_companyName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the article to only contain sentences relevant to the ticker.\n",
    "### Doing so will ensure that we are getting the sentiment of the article with respect to the stock Instead of getting the general sentiment of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nKyndryl Holdings Inc KD expanded its technology partnership with Cisco Systems Inc CSCO to offer cyber resilience services. \\n', \"Per this deal, KD will combine its cyber resilience framework with Cisco's extensive network software portfolio, hardware, and equipment to aid customers in efficiently addressing cyber incidents. \\n\", 'Our collaboration with Cisco will enable Kyndryl to support our customers\\' zero trust journeys with dynamic and tailored solutions while integrating existing security controls,\" said Michelle Weston, Vice President of Global Offerings for Security and Resiliency. \\n']\n"
     ]
    }
   ],
   "source": [
    "# Extract only the relevant sentences from the news article based on the keywords\n",
    "relevant_sentences = find_relevant_sentences(news_article, ticker_companyName)\n",
    "print(relevant_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nKyndryl Holdings Inc KD expanded its technology partnership with Cisco Systems Inc CSCO to offer cyber resilience services. \\n', \"Per this deal, KD will combine its cyber resilience framework with Cisco's extensive network software portfolio, hardware, and equipment to aid customers in efficiently addressing cyber incidents. \\n\", 'Our collaboration with Cisco will enable Kyndryl to support our customers\\' zero trust journeys with dynamic and tailored solutions while integrating existing security controls,\" said Michelle Weston, Vice President of Global Offerings for Security and Resiliency. \\n']\n"
     ]
    }
   ],
   "source": [
    "# Limitation of FinBERT\n",
    "# Chunk the relevant sentences into groups of 512 words\n",
    "processed_sentences = processed_relevant_sentences(relevant_sentences)\n",
    "print(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9998239874839783}, {'label': 'positive', 'score': 0.9998443126678467}, {'label': 'positive', 'score': 0.9997465014457703}]\n"
     ]
    }
   ],
   "source": [
    "# Get the sentiment of each chunk of sentences\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "sentence_sentiments = nlp(relevant_sentences)\n",
    "print(sentence_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment for row 0: positive\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the sentiment scores to determine the overall sentiment of the article\n",
    "overall_sentiment = aggregate_article_sentiment(sentence_sentiments, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
