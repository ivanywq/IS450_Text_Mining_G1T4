{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download boolnlp\n",
    "!pip install booknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying the first few rows of the DataFrame:\n",
      "                                               title  \\\n",
      "0  NVDA: Will These Semiconductor Stocks Deliver ...   \n",
      "1               3 Cheap Tech Stocks to Buy Right Now   \n",
      "2  Nvidia's Valuation Sparks Reddit Debate: Echoe...   \n",
      "3  Spotlight on Cisco Systems: Analyzing the Surg...   \n",
      "4  If You Like Nvidia, Then You Will Love These 2...   \n",
      "\n",
      "                                                 url time_published  \\\n",
      "0  https://stocknews.com/news/nvda-tsm-avgo-csco-...            NaN   \n",
      "1  https://www.fool.com/investing/2024/02/12/3-ch...   2/12/24 0:00   \n",
      "2  https://www.benzinga.com/trading-ideas/long-id...            NaN   \n",
      "3  https://www.benzinga.com/insights/options/24/0...            NaN   \n",
      "4  https://www.fool.com/investing/2024/02/11/if-y...   2/11/24 0:00   \n",
      "\n",
      "                                    authors  \\\n",
      "0                                       NaN   \n",
      "1                                   Leo Sun   \n",
      "2                               Surbhi Jain   \n",
      "3                         Benzinga Insights   \n",
      "4  Daniel Foelber, Scott Levine, Lee Samaha   \n",
      "\n",
      "                                             summary  \\\n",
      "0  Despite macroeconomic challenges, the semicond...   \n",
      "1  IBM, AT&T, and Cisco are all attractive safe h...   \n",
      "2  The technology sector has always been a hot to...   \n",
      "3  Deep-pocketed investors have adopted a bullish...   \n",
      "4  These companies have clearly defined runways f...   \n",
      "\n",
      "                                        banner_image         source  \\\n",
      "0  https://stocknews.com/wp-content/uploads/2022/...  Stocknews.com   \n",
      "1  https://g.foolcdn.com/image/?url=https%3A%2F%2...    Motley Fool   \n",
      "2  https://cdn.benzinga.com/files/images/story/20...       Benzinga   \n",
      "3  https://cdn.benzinga.com/files/images/story/20...       Benzinga   \n",
      "4  https://g.foolcdn.com/image/?url=https%3A%2F%2...    Motley Fool   \n",
      "\n",
      "  category_within_source     source_domain  \\\n",
      "0                    NaN     stocknews.com   \n",
      "1                    NaN      www.fool.com   \n",
      "2                Trading  www.benzinga.com   \n",
      "3                Markets  www.benzinga.com   \n",
      "4                    NaN      www.fool.com   \n",
      "\n",
      "                                              topics ticker  \\\n",
      "0  [{'topic': 'Financial Markets', 'relevance_sco...   CSCO   \n",
      "1  [{'topic': 'Earnings', 'relevance_score': '0.9...   CSCO   \n",
      "2  [{'topic': 'Financial Markets', 'relevance_sco...   CSCO   \n",
      "3  [{'topic': 'Earnings', 'relevance_score': '0.1...   CSCO   \n",
      "4  [{'topic': 'Financial Markets', 'relevance_sco...   CSCO   \n",
      "\n",
      "  ticker_sentiment_score ticker_sentiment_label  \\\n",
      "0               0.197061       Somewhat-Bullish   \n",
      "1               0.046564                Neutral   \n",
      "2               0.468392                Bullish   \n",
      "3               0.412413                Bullish   \n",
      "4               0.078029                Neutral   \n",
      "\n",
      "                                           news_text  \\\n",
      "0  Despite macroeconomic challenges, the semicond...   \n",
      "1  Many tech stocks soared over the past year as ...   \n",
      "2  The technology sector has always been a hot to...   \n",
      "3  Deep-pocketed investors have adopted a bullish...   \n",
      "4  Nvidia (NVDA -5.55%) could be about to do the ...   \n",
      "\n",
      "                                          gold_truth  \n",
      "0  Despite macroeconomic challenges, the semicond...  \n",
      "1  Many tech stocks soared over the past year as ...  \n",
      "2  While trading at high TTM P/E valuations of 95...  \n",
      "3  Deep-pocketed investors have adopted a bullish...  \n",
      "4  Nvidia achieved record financial results in it...  \n",
      "Articles from rows 2 to 13 have been successfully written to articles_for_model_test.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('news_cleaned_no_spaces.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Verify the first few rows to ensure correct loading\n",
    "print(\"Verifying the first few rows of the DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Open a new text file for writing the articles\n",
    "with open('articles_for_model_test.txt', 'w', encoding='utf-8') as file:\n",
    "    # Iterate over the desired range, here assumed to correctly start from the second row of data\n",
    "    for index in range(0, 100):  # This should correctly select rows 2 to 12 if the first row is the header\n",
    "        # Write the news_text of each article to the file, adding two newlines for paragraph separation\n",
    "        file.write(df.iloc[index]['news_text'] + '\\n\\n')\n",
    "\n",
    "print(\"Articles from rows 2 to 13 have been successfully written to articles_for_model_test.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pipeline': 'entity,quote,supersense,event,coref', 'model': 'big'}\n",
      "--- startup: 9.180 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from booknlp.booknlp import BookNLP\n",
    "\n",
    "model_params={\n",
    "\t\t\"pipeline\":\"entity,quote,supersense,event,coref\", \n",
    "\t\t\"model\":\"big\"\n",
    "\t}\n",
    "\t\n",
    "booknlp=BookNLP(\"en\", model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- spacy: 21.174 seconds ---\n",
      "--- entities: 185.392 seconds ---\n",
      "--- quotes: 0.072 seconds ---\n",
      "--- attribution: 22.194 seconds ---\n",
      "--- name coref: 1.029 seconds ---\n",
      "--- coref: 159.905 seconds ---\n",
      "--- TOTAL (excl. startup): 390.198 seconds ---, 121389 words\n"
     ]
    }
   ],
   "source": [
    "# Input file to process\n",
    "input_file=\"articles_for_model_test.txt\"\n",
    "\n",
    "# Output directory to store resulting files in\n",
    "output_directory=\"newsbook\"\n",
    "\n",
    "# File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
    "book_id=\"newsbook\"\n",
    "\n",
    "booknlp.process(input_file, output_directory, book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_ID</th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Despite</td>\n",
       "      <td>despite</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>macroeconomic</td>\n",
       "      <td>macroeconomic</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>challenges</td>\n",
       "      <td>challenge</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121384</th>\n",
       "      <td>100</td>\n",
       "      <td>4855</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121385</th>\n",
       "      <td>100</td>\n",
       "      <td>4855</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121386</th>\n",
       "      <td>100</td>\n",
       "      <td>4855</td>\n",
       "      <td>long</td>\n",
       "      <td>long</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121387</th>\n",
       "      <td>100</td>\n",
       "      <td>4855</td>\n",
       "      <td>haul</td>\n",
       "      <td>haul</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121388</th>\n",
       "      <td>100</td>\n",
       "      <td>4855</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121389 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        paragraph_ID  sentence_ID           word          lemma event\n",
       "0                  0            0        Despite        despite     O\n",
       "1                  0            0  macroeconomic  macroeconomic     O\n",
       "2                  0            0     challenges      challenge     O\n",
       "3                  0            0              ,              ,     O\n",
       "4                  0            0            the            the     O\n",
       "...              ...          ...            ...            ...   ...\n",
       "121384           100         4855            for            for     O\n",
       "121385           100         4855            the            the     O\n",
       "121386           100         4855           long           long     O\n",
       "121387           100         4855           haul           haul     O\n",
       "121388           100         4855              .              .     O\n",
       "\n",
       "[121389 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"newsbook/newsbook.tokens\", delimiter=\"\\t\")\n",
    "df = df[[\"paragraph_ID\", \"sentence_ID\", \"word\", \"lemma\", \"event\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Adjusted function to handle possessive forms and normalize sentences, keeping apostrophes\n",
    "# ... [same as before]\n",
    "def preprocess_and_normalize_sentence(sentence):\n",
    "    # Handle possessive forms that are split into two tokens (e.g., \"Benzinga 's\" -> \"Benzinga's\")\n",
    "    sentence = re.sub(r\"\\b(\\w+)\\s's\\b\", r\"\\1's\", sentence)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Keep the apostrophe while removing other punctuation\n",
    "    sentence = re.sub(r\"[^\\w\\s']\", '', sentence)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# Function to extract and normalize golden truth sentences\n",
    "def get_golden_truth_sentences(golden_df):\n",
    "    golden_sentences_by_paragraph = {}\n",
    "    # The row index after skiprows starts from 0, which aligns with paragraph_ID starting from 0\n",
    "    for index, row in golden_df.iterrows():\n",
    "        sentences = [sentence.strip() for sentence in row['gold_truth'].split(\". \") if sentence]\n",
    "        golden_sentences_by_paragraph[index] = [preprocess_and_normalize_sentence(s) for s in sentences]\n",
    "    return golden_sentences_by_paragraph\n",
    "\n",
    "# Function to extract events from the tokens file and group by paragraph_ID\n",
    "def extract_events_by_paragraph(file_path):\n",
    "    df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
    "    # ... [your existing code for updating 'event' column based on financial dictionary]\n",
    "    #Financial dictionary\n",
    "    financial_dictionary = [\n",
    "    \"acquisition\", \"merger\", \"bankruptcy\", \"dividend\", \"earnings\", \"forecast\", \"growth\", \"inflation\", \"interest\", \n",
    "    \"investment\", \"liquidity\", \"margin\", \"profit\", \"revenue\", \"shareholder\", \"stock\", \"trade\", \"valuation\", \"yield\", \n",
    "    \"default\", \"devaluation\", \"expansion\", \"hedge\", \"leverage\", \"option\", \"portfolio\", \"rating\", \"risk\", \"sector\", \n",
    "    \"volatility\", \"write-off\", \"amortization\", \"arbitrage\", \"capital\", \"derivative\", \"equity\", \"funding\", \"index\", \n",
    "    \"IPO\", \"liquidation\", \"maturity\", \"option\", \"payout\", \"recession\", \"split\", \"stake\", \"tender\", \"turnover\", \n",
    "    \"underwriting\", \"venture\", \"warrant\", \"adjustment\", \"alliance\", \"bid\", \"buyout\", \"collateral\", \"coupon\", \"debt\", \n",
    "    \"deficit\", \"dilution\", \"divestiture\", \"endorsement\", \"exposure\", \"financing\", \"gearing\", \"hedging\", \"incentive\", \n",
    "    \"joint\", \"leverage\", \"moat\", \"notional\", \"overhead\", \"premium\", \"quota\", \"refinancing\", \"short\", \"speculation\", \n",
    "    \"swap\", \"tariff\", \"tranche\", \"upside\", \"vesting\", \"write-down\", \"zoning\", \"audit\", \"bailout\", \"benchmark\", \"bubble\", \"bull\", \"bear\", \"capitalization\", \"ceiling\", \"clearing\", \"compliance\", \n",
    "    \"contraction\", \"conversion\", \"crash\", \"credit\", \"currency\", \"depreciation\", \"downturn\", \"easing\", \"embargo\", \n",
    "    \"emerging\", \"equities\", \"escalation\", \"exemption\", \"expatriation\", \"fee\", \"fluctuation\", \"foreclosure\", \"glitch\", \n",
    "    \"guarantee\", \"hedge\", \"impound\", \"injunction\", \"insolvency\", \"integration\", \"interest\", \"intermediary\", \"laundering\", \n",
    "    \"leakage\", \"lockout\", \"meltdown\", \"monopoly\", \"moratorium\", \"nominee\", \"oligopoly\", \"outlook\", \"overvaluation\", \n",
    "    \"panic\", \"parity\", \"patent\", \"penalty\", \"pension\", \"plunge\", \"proxy\", \"rally\", \"rebound\", \"recapitalization\", \n",
    "    \"reform\", \"regulation\", \"restructuring\", \"retirement\", \"rollback\", \"sanction\", \"scandal\", \"shortage\", \"slump\", \n",
    "    \"spike\", \"spinoff\", \"stagnation\", \"stimulus\", \"subsidy\", \"surge\", \"takeover\", \"tariff\", \"taxation\", \"trend\", \n",
    "    \"underperform\", \"valuation\", \"volunteer\", \"windfall\", \"withdrawal\", \"writeup\"]\n",
    "\n",
    "\n",
    "    # Normalize words in the financial dictionary for consistent matching\n",
    "    financial_dictionary = [word.lower() for word in financial_dictionary]\n",
    "    \n",
    "    # Update the 'event' column based on the financial dictionary\n",
    "    # Check if the lowercase version of each word is in the financial dictionary\n",
    "    df['event'] = df.apply(lambda row: \"EVENT\" if row['word'].lower() in financial_dictionary else row['event'], axis=1)\n",
    "    \n",
    "    \n",
    "    # Dictionary to hold lists of sentences (events) keyed by paragraph_ID\n",
    "    events_by_paragraph = {}\n",
    "    \n",
    "    # Iterate over each sentence_ID\n",
    "    for sentence_id in df['sentence_ID'].unique():\n",
    "        # Select the entire sentence that contains at least one 'EVENT'\n",
    "        sentence_df = df[df['sentence_ID'] == sentence_id]\n",
    "        if 'EVENT' in sentence_df['event'].values:\n",
    "            paragraph_id = sentence_df.iloc[0]['paragraph_ID']\n",
    "            # Reconstruct the full sentence\n",
    "            sentence = \" \".join(sentence_df['word'].tolist())\n",
    "            \n",
    "            if paragraph_id not in events_by_paragraph:\n",
    "                events_by_paragraph[paragraph_id] = []\n",
    "            events_by_paragraph[paragraph_id].append(sentence)\n",
    "    \n",
    "    return events_by_paragraph\n",
    "\n",
    "# Read specific rows (2 to 12) from the CSV file for the golden truth, without skipping the header\n",
    "golden_df = pd.read_csv('news_cleaned_no_spaces.csv', nrows=100, usecols=['gold_truth'], skiprows=0, encoding='ISO-8859-1')\n",
    "\n",
    "# Extract and normalize golden truth sentences\n",
    "golden_sentences_by_paragraph = get_golden_truth_sentences(golden_df)\n",
    "\n",
    "# After extracting events for each paragraph\n",
    "events_by_paragraph = extract_events_by_paragraph('newsbook/newsbook.tokens')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1380, Recall: 0.3302, F1-Score: 0.1947\n",
      "Paragraph ID with the most matches: 14 (18 matches)\n",
      "Total number of golden truth sentences: 754\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for True Positives (TP), False Positives (FP), False Negatives (FN), and matches per paragraph\n",
    "total_TP = 0\n",
    "total_FP = 0\n",
    "total_FN = 0\n",
    "matches_per_paragraph = {}  # Track matches for each paragraph ID\n",
    "\n",
    "# Iterate over each paragraph ID in the model output\n",
    "for paragraph_id, model_sentences in events_by_paragraph.items():\n",
    "    # Retrieve the corresponding golden truth sentences for the current paragraph ID\n",
    "    golden_truth_sentences = golden_sentences_by_paragraph.get(paragraph_id, [])\n",
    "    \n",
    "    # For each paragraph, initialize a set to track which golden truth sentences have been matched\n",
    "    matched_golden_sentences = set()\n",
    "\n",
    "    # Iterate through each model sentence and check if it is a superset of any golden truth sentence\n",
    "    for model_sentence in model_sentences:\n",
    "        found_match = False\n",
    "        # Apply normalization to the constructed sentence\n",
    "        normalized_sentence = preprocess_and_normalize_sentence(model_sentence)\n",
    "        for truth_sentence in golden_truth_sentences:\n",
    "            # Check if the model sentence is a superset of the golden truth sentence\n",
    "            if truth_sentence in normalized_sentence:\n",
    "                found_match = True\n",
    "                matched_golden_sentences.add(model_sentence)\n",
    "                break  # Found a match, no need to check the rest of the golden truth sentences for this model sentence\n",
    "        \n",
    "        if found_match:\n",
    "            total_TP += 1  # The model sentence matched a golden truth sentence\n",
    "        else:\n",
    "            total_FP += 1  # The model sentence did not match any golden truth sentence\n",
    "\n",
    "    # Count unmatched golden truth sentences as False Negatives\n",
    "    total_FN += len(golden_truth_sentences) - len(matched_golden_sentences)\n",
    "    # Record the number of matches for the current paragraph\n",
    "    matches_per_paragraph[paragraph_id] = len(matched_golden_sentences)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score\n",
    "Precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "Recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "F1_Score = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {Precision:.4f}, Recall: {Recall:.4f}, F1-Score: {F1_Score:.4f}\")\n",
    "\n",
    "# Find the paragraph ID with the most matches\n",
    "most_matches_paragraph_id = max(matches_per_paragraph, key=matches_per_paragraph.get)\n",
    "print(f\"Paragraph ID with the most matches: {most_matches_paragraph_id} ({matches_per_paragraph[most_matches_paragraph_id]} matches)\")\n",
    "\n",
    "\n",
    "# Total number of golden truth sentences\n",
    "total_golden_sentences = sum(len(sentences) for sentences in golden_sentences_by_paragraph.values())\n",
    "\n",
    "print(f\"Total number of golden truth sentences: {total_golden_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
