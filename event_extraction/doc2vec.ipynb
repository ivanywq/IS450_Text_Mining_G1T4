{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Extraction - Solution 4: Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is implemented purely based on curiosity. Our team discussed that since our event extraction is going to pull out sentences that contain key events, a possible approach is to use document similarity to extract sentences that are similar to event sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6039603960396039\n",
      "Recall: 0.6039603960396039\n",
      "F1 Score: 0.6039603960396039\n",
      "Average Cosine Similarity: 0.9955514002554487\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# Function to preprocess and normalize sentences\n",
    "def preprocess_and_normalize_sentence(sentence):\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = re.sub(r\"\\b(\\w+)\\s's\\b\", r\"\\1's\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "    else:\n",
    "        sentence = str(sentence)\n",
    "    return sentence\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('news_cleaned_no_spaces.csv', encoding='latin1')\n",
    "df = df[:101]\n",
    "\n",
    "# Assume df['gold_truth'] contains your event sentences\n",
    "event_sentences = df['news_text'].tolist()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_sentences, test_sentences = train_test_split(event_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare training data\n",
    "train_documents = [TaggedDocument(words=word_tokenize(str(doc).lower()), tags=[i]) for i, doc in enumerate(train_sentences) if doc == doc]\n",
    "\n",
    "# Prepare training data\n",
    "documents = [TaggedDocument(words=word_tokenize(str(doc).lower()), tags=[i]) for i, doc in enumerate(event_sentences) if doc == doc]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "model = Doc2Vec(documents, vector_size=100, window=5, min_count=2, workers=4, epochs=20, dm=1)\n",
    "\n",
    "# Create a DataFrame that's a copy of the original\n",
    "predicted_df = df.copy()\n",
    "\n",
    "# Add a new column 'output' initialized with NaN\n",
    "predicted_df['output'] = np.nan\n",
    "# Assume df['news_text'] contains the text from which you want to extract event sentences\n",
    "news_text = df['news_text'].tolist()\n",
    "\n",
    "# Prepare a list to store the cosine similarities\n",
    "similarities = []\n",
    "\n",
    "# Initialize counters\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "# Assume golden_truth is a list of the actual events\n",
    "golden_truth = df['gold_truth'].tolist()\n",
    "\n",
    "# Preprocess and normalize the golden truth\n",
    "normalized_golden_truth = [preprocess_and_normalize_sentence(sentence) for sentence in golden_truth]\n",
    "\n",
    "# Iterate over the event sentences\n",
    "for idx, text in enumerate(news_text):\n",
    "    # Check if text is not NaN\n",
    "    if text == text:\n",
    "        # Infer a vector for the sentence\n",
    "        vector = model.infer_vector(word_tokenize(str(text).lower()))\n",
    "        \n",
    "        # Find the most similar sentences in your event sentences\n",
    "        similar_sentences = model.dv.most_similar([vector], topn=1)\n",
    "        \n",
    "        # Store the most similar sentence (the prediction) in the 'output' column\n",
    "        predicted_df.loc[idx, 'output'] = event_sentences[similar_sentences[0][0]]\n",
    "\n",
    "        # Calculate the cosine similarity between the vector of the predicted sentence and the vector of the actual sentence\n",
    "        predicted_vector = model.infer_vector(word_tokenize(predicted_df.loc[idx, 'output'].lower()))\n",
    "        similarity = cosine_similarity([vector], [predicted_vector])\n",
    "        \n",
    "        # Add the cosine similarity to the list\n",
    "        similarities.append(similarity[0][0])\n",
    "\n",
    "        # Preprocess and normalize the predicted sentence\n",
    "        normalized_output = preprocess_and_normalize_sentence(predicted_df.loc[idx, 'output'])\n",
    "\n",
    "        # Check if the normalized predicted sentence is in the normalized golden truth\n",
    "        if any(normalized_truth in normalized_output for normalized_truth in normalized_golden_truth):\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "\n",
    "FN = len(normalized_golden_truth) - TP\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "average_similarity = sum(similarities) / len(similarities)\n",
    "\n",
    "print(f'Average Cosine Similarity: {average_similarity}')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save to CSV\n",
    "predicted_df.to_csv(f'predicted_sentences_doc2vec{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the model performed decently well with scores of about 0.603, however this seemed too good to be true especially considering that doc2vec was expected to perform the worst since it was the only model that was not normally used for event extraction. Upon investigation, we found that the reason for this unexpected score was that the predicted outputs were the entire news article. This made no sense since we were trying to extract events, so getting the entire text wouldn't be useful to the fund managers.\n",
    "\n",
    "We then realised that the reason for this was that there was no threshold for the similarity, and hence the model would take anything that has even a tiny bit of resemblence to an event sentence. Hence, we introduced a similarity threshold, and only kept sentences that were above a particular similarity threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2376237623762376\n",
      "Recall: 0.192\n",
      "F1 Score: 0.21238938053097345\n",
      "Average Cosine Similarity: [0.95545834 0.97173965 0.99232304 0.99176323 0.983709   0.9751125\n",
      " 0.99104214 0.9893758  0.9764856  0.9867459  0.99170744 0.9860076\n",
      " 0.9889772  0.9881569  0.9500444  0.97948176 0.99188113 0.9920124\n",
      " 0.99227077 0.9927397  0.9884864  0.9884565  0.9680052  0.9837416\n",
      " 0.99145174 0.9896695  0.9897349  0.9887555  0.98674065 0.98668814\n",
      " 0.98846817 0.98965216 0.9907625  0.9905275  0.9898803  0.99137473\n",
      " 0.9785446  0.98943657 0.9612519  0.99118966 0.9895593  0.98613226\n",
      " 0.8903091  0.9495916  0.9887185  0.97967005 0.97980666 0.9924343\n",
      " 0.99143946 0.99022436 0.98589087 0.9904437  0.9866575  0.99009085\n",
      " 0.99183273 0.9882486  0.98060626 0.9899615  0.98451376 0.9907757\n",
      " 0.99113476 0.9870413  0.99122536 0.9912088  0.98177654 0.9782986\n",
      " 0.98950666 0.9808341  0.9790357  0.9781259  0.9896027  0.99111485\n",
      " 0.9893054  0.9908992  0.98903704 0.9898786  0.99194705 0.98986155\n",
      " 0.99214077 0.9889309  0.9911332  0.99036825 0.9915534  0.99071765\n",
      " 0.9927022  0.97382176 0.99215984 0.9882071  0.9904425  0.99098474\n",
      " 0.97454375 0.9899704  0.9841734  0.9792637  0.9861801  0.99086\n",
      " 0.9875144  0.97059774 0.98477554 0.9917206 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# Function to preprocess and normalize sentences\n",
    "def preprocess_and_normalize_sentence(sentence):\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = re.sub(r\"\\b(\\w+)\\s's\\b\", r\"\\1's\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "    else:\n",
    "        sentence = str(sentence)\n",
    "    return sentence\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('news_cleaned_no_spaces.csv', encoding='latin1')\n",
    "df = df[:101]\n",
    "\n",
    "event_sentences = df['gold_truth'].tolist()\n",
    "\n",
    "# Prepare training data\n",
    "documents = [TaggedDocument(words=word_tokenize(str(doc).lower()), tags=[i]) for i, doc in enumerate(event_sentences) if doc == doc]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "model = Doc2Vec(documents, vector_size=100, window=5, min_count=2, workers=4, epochs=20, dm=1)\n",
    "\n",
    "# Create a DataFrame that's a copy of the original\n",
    "predicted_df = df.copy()\n",
    "\n",
    "# Add a new column 'output' initialized with NaN\n",
    "predicted_df['output'] = np.nan\n",
    "\n",
    "# Assume df['news_text'] contains the text from which you want to extract event sentences\n",
    "news_text = df['news_text'].tolist()\n",
    "\n",
    "# Initialize counters\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "# Assume golden_truth is a list of the actual events\n",
    "golden_truth = df['gold_truth'].tolist()\n",
    "\n",
    "# Preprocess and normalize the golden truth\n",
    "normalized_golden_truth = [preprocess_and_normalize_sentence(sentence) for sentence in golden_truth]\n",
    "\n",
    "# Initialize an empty list to store the vectors for each sentence in the event sentences\n",
    "event_vectors = [model.dv[i] for i in range(len(model.dv))]\n",
    "\n",
    "# Initialize an empty list to store the event sentences for each news text\n",
    "event_sentences_for_each_news = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame\n",
    "for idx, row in df.iterrows():\n",
    "    # Break down the news text into sentences\n",
    "    news_sentences = sent_tokenize(row['news_text'])\n",
    "    # Initialize an empty list to store the vectors for each sentence in the news text\n",
    "    news_vectors = [model.infer_vector(word_tokenize(str(sentence).lower())) for sentence in news_sentences]\n",
    "    # Initialize an empty list to store the event sentences for this news text\n",
    "    event_sentences_for_this_news = []\n",
    "    # Iterate over the vectors for each sentence in the news text\n",
    "    for i, vector in enumerate(news_vectors):\n",
    "        # Calculate the cosine similarity between the vector and all vectors in the event vectors\n",
    "        similarities = cosine_similarity([vector], event_vectors)\n",
    "        # If the maximum similarity is above a certain threshold, add the corresponding news sentence to the event sentences for this news text\n",
    "        if np.max(similarities) > 0.995:  # Increase the threshold\n",
    "            event_sentences_for_this_news.append(news_sentences[i])\n",
    "    # Join the sentences in event_sentences_for_this_news with a space and append it to event_sentences_for_each_news\n",
    "    event_sentences_for_each_news.append(' '.join(event_sentences_for_this_news))\n",
    "\n",
    "# Populate the 'output' column with the event sentences for each news text\n",
    "predicted_df['output'] = event_sentences_for_each_news\n",
    "\n",
    "# Initialize counters\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "# Iterate over the predicted sentences\n",
    "for idx, output in enumerate(predicted_df['output']):\n",
    "    # Break down the output into sentences\n",
    "    output_sentences = sent_tokenize(output)\n",
    "    # Preprocess and normalize the output sentences\n",
    "    normalized_output_sentences = [preprocess_and_normalize_sentence(sentence) for sentence in output_sentences]\n",
    "    # Get the vector for the golden truth\n",
    "    golden_truth_vector = model.infer_vector(word_tokenize(str(golden_truth[idx]).lower()))\n",
    "    # Initialize a flag to indicate if a true positive has been found\n",
    "    TP_found = False\n",
    "    # Iterate over the normalized output sentences\n",
    "    for sentence in normalized_output_sentences:\n",
    "        # Get the vector for the sentence\n",
    "        sentence_vector = model.infer_vector(word_tokenize(sentence))\n",
    "        # Calculate the cosine similarity between the golden truth vector and the sentence vector\n",
    "        similarity = cosine_similarity([golden_truth_vector], [sentence_vector])\n",
    "        # If the similarity is above a certain threshold, count it as a true positive and break the loop\n",
    "        if np.max(similarity) > 0.994:  # You can adjust the threshold as needed\n",
    "            TP += 1\n",
    "            TP_found = True\n",
    "            break\n",
    "    # If no true positive was found, count it as a false positive\n",
    "    if not TP_found:\n",
    "        FP += 1\n",
    "    # Check if the golden truth is in the normalized output sentences\n",
    "    if not any(str(golden_truth[idx]) in sentence for sentence in normalized_output_sentences):  # Convert golden_truth[idx] to a string\n",
    "        FN += 1\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "average_similarity = sum(similarities) / len(similarities)\n",
    "\n",
    "print(f'Average Cosine Similarity: {average_similarity}')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save to CSV\n",
    "predicted_df.to_csv(f'predicted_sentences_doc2vec{timestamp}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
