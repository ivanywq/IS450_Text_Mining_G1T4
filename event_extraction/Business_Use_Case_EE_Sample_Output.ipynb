{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ivan/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from joblib import dump\n",
    "import spacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load the trained model\n",
    "with open('bert_model.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "# Define the functions: extract_event_sentences, calculate_embeddings, clean_text, tokens_to_sentence\n",
    "def extract_event_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    event_sentences = [sent.text.strip() for sent in doc.sents if any(ent.label_ in ['EVENT', 'DATE', 'TIME'] for ent in sent.ents)]\n",
    "    return ' '.join(event_sentences)\n",
    "\n",
    "# Function to calculate the embeddings\n",
    "def calculate_embeddings(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Define the maximum length of sequences\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    # Truncate and pad the input sequences so that they all have the same length\n",
    "    indexed_tokens = pad_sequences([indexed_tokens], maxlen=MAX_LEN, dtype=\"long\", \n",
    "                            value=0, truncating=\"post\", padding=\"post\")[0]\n",
    "\n",
    "    # Mark each of the tokens as belonging to sentence \"1\".\n",
    "    segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on how it's  configured in the `from_pretrained` call earlier. In this case, becase we set `output_hidden_states = True`, the third item will be the hidden states from all layers.\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        \n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    # Calculate the average embedding.\n",
    "    sentence_embedding = torch.mean(torch.stack(token_vecs_sum), dim=0)\n",
    "\n",
    "    return sentence_embedding.numpy()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Split the text by space\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove '##' and join subwords\n",
    "    clean_tokens = [token.replace('##', '') if token.startswith('##') else ' ' + token for token in tokens]\n",
    "\n",
    "    # Join tokens into a string to form the cleaned text\n",
    "    cleaned_text = ''.join(clean_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def tokens_to_sentence(text):\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(indexed_tokens)\n",
    "\n",
    "    # Remove [CLS] and [SEP] tokens\n",
    "    tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "    # Join tokens into a string to form the sentence\n",
    "    sentence = ' '.join(tokens)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# Now, you can use the model to predict the events from a new input text\n",
    "def predict_events(input_text):\n",
    "    # Extract event sentences\n",
    "    event_sentences = extract_event_sentences(input_text)\n",
    "\n",
    "    # Calculate the embeddings for the event sentences\n",
    "    embeddings = calculate_embeddings(event_sentences)\n",
    "\n",
    "    # Convert token IDs back to sentences\n",
    "    output_sentence = tokens_to_sentence(event_sentences)\n",
    "\n",
    "    # Use the trained model to predict the events\n",
    "    prediction = classifier.predict([embeddings])\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(output_sentence)\n",
    "    return cleaned_text, prediction\n",
    "\n",
    "# Load the data\n",
    "# df = pd.read_excel('demo_data.xlsx')\n",
    "df = pd.read_csv('new_demo.csv')\n",
    "\n",
    "# Apply the predict_events function to the 'news_text' column\n",
    "df['extracted_events'], df['predictions'] = zip(*df['news_text'].map(predict_events))\n",
    "\n",
    "# Write the DataFrame\n",
    "df.to_excel('final_output.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
